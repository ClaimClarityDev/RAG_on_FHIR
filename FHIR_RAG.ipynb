{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ed96ff476fd1900",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# FHIR on RAG\n",
    "\n",
    "This notebook is loading FHIR resources into a vector store and then using that to help prompt an LLM to answer questions about the data. To do that, it first flattens the FHIR resources into text files. It then uses [LlamaIndex](https://www.llamaindex.ai/) to load the text files into an in-memory vector store. Then it calls out to a LLama 2 running locally using [Ollama](https://ollama.ai/) using different [strategies](https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/root.html) for combining the FHIR with the question into the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcbc79e952c4890",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Some constants to use throughout \n",
    "\n",
    "in_file_glob = './working/raw_fhir/*.json'\n",
    "flat_file_path = './working/flat'\n",
    "vector_store_file_path = './working/vector_store'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b74f4239063888",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Flatten FHIR\n",
    "\n",
    "This is going to read in any JSON files in the `in_file_glob`. It assumes that each file is a FHIR Bundle. It will first pull out the Patient resource and extract some key information, like name, from it to include in the text files it will create per resource. This helps the RAG know which patient a resource goes with. It then flattens each resource in the bundle. \n",
    "\n",
    "Flattening it means that it creates a path of all the attribute names from the root of the resource to each value. In the process it splits any camel case words into multiple words. Finally, it writes this out to a text file in the structure of:\n",
    "``` [path name] is [value]. ```\n",
    "This creates a semi-english version of the resource that can be turned into a vector by the embedding. \n",
    "\n",
    "**To use this project,** you will need to create the working and raw_fhir directories and populate raw_fhir with FHIR Bundles. I used [Synthea](https://synthea.mitre.org/) to generate synthetic data in my testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6665eba975cf558",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "camel_pattern1 = re.compile(r'(.)([A-Z][a-z]+)')\n",
    "camel_pattern2 = re.compile(r'([a-z0-9])([A-Z])')\n",
    "\n",
    "\n",
    "def split_camel(text):\n",
    "    new_text = camel_pattern1.sub(r'\\1 \\2', text)\n",
    "    new_text = camel_pattern2.sub(r'\\1 \\2', new_text)\n",
    "    return new_text\n",
    "\n",
    "\n",
    "def handle_special_attributes(attrib_name, value):\n",
    "    if attrib_name == 'resource Type':\n",
    "        return split_camel(value)\n",
    "    return value\n",
    "\n",
    "\n",
    "def flatten_fhir(nested_json):\n",
    "    out = {}\n",
    "\n",
    "    def flatten(json_to_flatten, name=''):\n",
    "        if type(json_to_flatten) is dict:\n",
    "            for sub_attribute in json_to_flatten:\n",
    "                flatten(json_to_flatten[sub_attribute], name + split_camel(sub_attribute) + ' ')\n",
    "        elif type(json_to_flatten) is list:\n",
    "            for i, sub_json in enumerate(json_to_flatten):\n",
    "                flatten(sub_json, name + str(i) + ' ')\n",
    "        else:\n",
    "            attrib_name = name[:-1]\n",
    "            out[attrib_name] = handle_special_attributes(attrib_name, json_to_flatten)\n",
    "\n",
    "    flatten(nested_json)\n",
    "    return out\n",
    "\n",
    "\n",
    "def filter_for_patient(entry):\n",
    "    return entry['resource']['resourceType'] == \"Patient\"\n",
    "\n",
    "\n",
    "def find_patient(bundle):\n",
    "    patients = list(filter(filter_for_patient, bundle['entry']))\n",
    "    if len(patients) < 1:\n",
    "        raise Exception('No Patient found in bundle!')\n",
    "    else:\n",
    "        patient = patients[0]['resource']\n",
    "\n",
    "        patient_id = patient['id']\n",
    "        first_name = patient['name'][0]['given'][0]\n",
    "        last_name = patient['name'][0]['family']\n",
    "\n",
    "        return {'PatientFirstName': first_name, 'PatientLastName': last_name, 'PatientID': patient_id}\n",
    "\n",
    "\n",
    "def flat_to_string(flat_entry):\n",
    "    output = ''\n",
    "\n",
    "    for attrib in flat_entry:\n",
    "        output += f'{attrib} is {flat_entry[attrib]}. '\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def flatten_bundle(bundle_file_name):\n",
    "    file_name = bundle_file_name[bundle_file_name.rindex('/') + 1:bundle_file_name.rindex('.')]\n",
    "    with open(bundle_file_name) as raw:\n",
    "        bundle = json.load(raw)\n",
    "        patient = find_patient(bundle)\n",
    "        flat_patient = flatten_fhir(patient)\n",
    "        for i, entry in enumerate(bundle['entry']):\n",
    "            flat_entry = flatten_fhir(entry['resource'])\n",
    "            with open(f'{flat_file_path}/{file_name}_{i}.txt', 'w') as out_file:\n",
    "                out_file.write(f'{flat_to_string(flat_patient)}\\n{flat_to_string(flat_entry)}')\n",
    "\n",
    "\n",
    "if not os.path.exists(flat_file_path):\n",
    "    os.mkdir(flat_file_path)\n",
    "\n",
    "for file in glob.glob(in_file_glob):\n",
    "    flatten_bundle(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9af9519682334df",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Setup the Gen AI with RAG\n",
    "\n",
    "This section will use LlamaIndex to construct the vector store and tie to the LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install llama-index\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "I tried a couple of different models for doing the embedding, i.e. turning the flattened FHIR text into vectors. I would like to experement with others, but haven't had time. In the end, `BAAI/bge-large-en-v1.5` was too big for me to run on my local, so I did most of my testing with `BAAI/bge-small-en-v1.5`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cad9e789f424c411"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a47aefc7f6f832e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "\n",
    "# loads BAAI/bge-small-en\n",
    "# embed_model = HuggingFaceEmbedding()\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\")\n",
    "\n",
    "# embed_model = HuggingFaceEmbedding(model_name=\"medicalai/ClinicalBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cf86c33ac94bbd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This code is to play with embedding if desired. It is not needed and can remain commented out.\n",
    "\n",
    "# embeddings = embed_model.get_text_embedding(\"Hello World!\")\n",
    "# print(len(embeddings))\n",
    "# print(embeddings[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6328dbe457d3dcde",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from llama_index.llms import Ollama\n",
    "\n",
    "# LLama 2 is running locally, using Ollama.\n",
    "llm = Ollama(model=\"llama2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7523591bb7324a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is a test prompt, just to prove that Ollama is working. It can remain commented out.\n",
    "\n",
    "# resp = llm.complete(\"Who is Paul Graham?\")\n",
    "# print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792169943ccb715",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext, VectorStoreIndex, SimpleDirectoryReader, SummaryIndex\n",
    "from llama_index import set_global_service_context\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1067bc55158f53b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This code loads the flat FHIR text files. \n",
    "\n",
    "documents = SimpleDirectoryReader(flat_file_path).load_data()\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10086bb49ca618b8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load those flat FHIR text files into the vector store.\n",
    "\n",
    "vector_index = VectorStoreIndex.from_documents(documents, show_progress=True)\n",
    "\n",
    "\n",
    "# if not os.path.exists(vector_store_file_path):\n",
    "#     os.mkdir(vector_store_file_path)\n",
    "# vector_index.vector_store.persist(f'{vector_store_file_path}/FHIR_RAG.vs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcde194f8d979e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I tried to play with summary indexes, but it took too long to get a response on my machine. \n",
    "\n",
    "# summary_index = SummaryIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5a5f3a941b5afa",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from llama_index.response.notebook_utils import display_response\n",
    "import logging\n",
    "import sys\n",
    "from IPython.core.display import Markdown\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9003e241adaae4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Actually do RAG\n",
    "\n",
    "This is the code block that actually asks the questions of the LLM. \n",
    "\n",
    "In my tests, I used synthetic FHIR generated by [Synthea](https://github.com/synthetichealth/synthea/wiki/Basic-Setup-and-Running). I had Synthea generate two patients and so I asked questions about each patient. If you are looking to replicate this work, you will need to change the patient names to match what ever data you have available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea468385e9d1c52a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def display_source_text(response):\n",
    "    for ind, source_node in enumerate(response.source_nodes):\n",
    "        display(Markdown(\"---\"))\n",
    "        display(Markdown(f\"**`Source Node {ind + 1}/{len(response.source_nodes)}`**\"))\n",
    "        text_md = (\n",
    "            f'**File:** {source_node.node.metadata[\"file_name\"]}<br>'\n",
    "            f'**Text:** {source_node.node.get_content().strip()}'\n",
    "        )\n",
    "        display(Markdown(text_md))\n",
    "\n",
    "\n",
    "def ask_question(index, response_mode, question, show_sources=False):\n",
    "    query_engine = index.as_query_engine(response_mode=response_mode, similarity_top_k=5)\n",
    "    response = query_engine.query(question)\n",
    "    display(Markdown(f'### Answer for {response_mode}'))\n",
    "    if show_sources:\n",
    "        display_source_text(response)\n",
    "    else:\n",
    "        display_response(response, show_source=False, show_metadata=False, show_source_metadata=False)\n",
    "\n",
    "\n",
    "def ask_question_all_modes(person, question):\n",
    "    display(Markdown(f'# Asking about {person}\\n<br>**Question:** {question}'))\n",
    "    ask_question(vector_index, 'no_text', question, show_sources=True)\n",
    "    ask_question(vector_index, 'simple_summarize', question)\n",
    "    ask_question(vector_index, 'compact', question)\n",
    "    ask_question(vector_index, 'refine', question)\n",
    "    ask_question(vector_index, 'tree_summarize', question)\n",
    "    ask_question(vector_index, 'accumulate', question)\n",
    "    ask_question(vector_index, 'compact_accumulate', question)\n",
    "\n",
    "\n",
    "ask_question_all_modes('Arnold',\n",
    "                       'What can you tell me about Arnold338 Wilkinson796 heart? For example, does he have hypertension?')\n",
    "ask_question_all_modes('Ashley', 'What can you tell me about Ashley34 Bergstrom287 allergies?')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
