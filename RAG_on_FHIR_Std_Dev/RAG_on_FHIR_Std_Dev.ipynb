{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# RAG on FHIR with Standard Deviation \n",
    "\n",
    "This notebook shows how to use Standard Deviation instead of K-Nearest Neighbor to find resources to supply for RAG. \n",
    "\n",
    "This notebook assumes you have already loaded data into the Knowledge Graph as per the notebook [FHIR_GRAPHS](https://github.com/samschifman/RAG_on_FHIR/blob/main/RAG_on_FHIR_with_KG/FHIR_GRAPHS.ipynb). This notebook is not intended to be run on its own. \n",
    "\n",
    "## Disclaimer\n",
    "Nothing provided here is guaranteed or warrantied to work. It is provided as is and has not been tested extensively. Using this notebook is at the risk of the user. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9aff3561dda71c9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Install and Import Libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2ff2c7088dbea76"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install sentence-transformers langchain neo4j"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1bb1ef3543ce1907"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain.vectorstores.neo4j_vector import Neo4jVector\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "# Imports from other local python files\n",
    "from NEO4J_Graph import Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Establish Database Connection\n",
    "\n",
    "The cell connects to the Neo4J instance. It relies on several environment variables. \n",
    "\n",
    "**PLEASE NOTE**: The variable have been changed to support multiple databases in the same instance. \n",
    "\n",
    "| Variable            | Description                          | Sample Value          |\n",
    "|---------------------|--------------------------------------|-----------------------|\n",
    "| FHIR_GRAPH_URL      | Where to find the instance of Neo4j. | bolt://localhost:7687 |\n",
    "| FHIR_GRAPH_USER     | The username for the database.       | neo4j                 |\n",
    "| FHIR_GRAPH_PASSWORD | The password for the database.       | password              |\n",
    "| FHIR_GRAPH_DATABASE | The name of the database instance.   | neo4j                 |"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc09c1ac2492c945"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NEO4J_URI = os.getenv('FHIR_GRAPH_URL')\n",
    "USERNAME = os.getenv('FHIR_GRAPH_USER')\n",
    "PASSWORD = os.getenv('FHIR_GRAPH_PASSWORD')\n",
    "DATABASE = os.getenv('FHIR_GRAPH_DATABASE')\n",
    "\n",
    "graph = Graph(NEO4J_URI, USERNAME, PASSWORD, DATABASE)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61be984f1c0b39a0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup Prompt Templates\n",
    "\n",
    "This cell sets the prompt template to use when calling the LLM."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fde7c499cf7ebabf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "my_prompt = '''\n",
    "System: The context below contains entries about the patient's healthcare. \n",
    "Please limit your answer to the information provided in the context. Do not make up facts. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "----------------\n",
    "{context}\n",
    "Human: {question}\n",
    "'''\n",
    "\n",
    "prompt = PromptTemplate.from_template(my_prompt)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bdbd1db6d87fd812"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define some helper methods for showing results. \n",
    "\n",
    "These methods are used later to make it easy to see what will be sent to the LLM and ask it questions. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e00f47ab282d6a85"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def show_similar(_question, _vector_index):\n",
    "    response = _vector_index.similarity_search(_question)\n",
    "    for page in response:\n",
    "        print(page.page_content)\n",
    "        print(' ')\n",
    "    print(' ')\n",
    "    print(f'Total number of responses: {len(response)}')\n",
    "\n",
    "\n",
    "def show_answers(_question, _vector_qa, number_of_times):\n",
    "    print('Answering...')\n",
    "    for i in range(number_of_times):\n",
    "        answer = _vector_qa.run(_question)\n",
    "        print(f'Answer {i+1}:')\n",
    "        print(answer)\n",
    "        print(' ')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5bcc743b17845e2c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pick the LLM model to use\n",
    "\n",
    "Ollama can run multiple models. I had the most luck with mistral. However, you could try others. The list of possible \n",
    "models is [here](https://ollama.ai/library)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22c1460fa686c228"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ollama_model = 'mistral'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2e8523deaa81f27"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Standard Deviation Wrapper\n",
    "\n",
    "This cell contains the code that wraps a Neo4jVector Store in code that will calculate the standard deviation and return results within 1 Std Dev of the closest value. By default it looks at the 1,000 nearest neighbors to calculate the Std Dev. \n",
    "\n",
    "Optionally, the wrapper can also deduplicate the values returned. This can be useful when using this in conjunction with a `retrieval_query`, which is not shown here. I leave it to you to combine this with what is shown in [FHIR_GRAPHS](https://github.com/samschifman/RAG_on_FHIR/blob/main/RAG_on_FHIR_with_KG/FHIR_GRAPHS.ipynb), but once they are combined being able to remove duplicates is key. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88592f6c5bbfcb14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from typing import List, Optional, Any, Iterable, Type\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_core.vectorstores import VectorStore, VST\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def deduplicate(existing_entries: List, doc: Document) -> Document:\n",
    "    entries = doc.page_content.split(\"Entry:\\n\")\n",
    "    entries = list(filter(lambda e: e and e not in existing_entries, entries))\n",
    "    existing_entries += entries\n",
    "    doc.page_content = 'Primary Entry:\\n' + 'Supporting Entry:\\n'.join(entries)\n",
    "    return doc\n",
    "\n",
    "\n",
    "class StandardDevNeo4jVector(VectorStore):\n",
    "\n",
    "    def __init__(self, vector_store: Neo4jVector, should_deduplicate:bool = False):\n",
    "        self._vector_store = vector_store\n",
    "        self._deduplicate = should_deduplicate\n",
    "\n",
    "    def add_texts(self, texts: Iterable[str], metadatas: Optional[List[dict]] = None, **kwargs: Any) -> List[str]:\n",
    "        return self._vector_store.add_texts(texts.metadatas, kwargs)\n",
    "\n",
    "    def similarity_search(self, query: str, k: int = 1000, **kwargs: Any) -> List[Document]:\n",
    "        docs = self._vector_store.similarity_search_with_score(query=query, k=k, **kwargs)\n",
    "        scores = [x[1] for x in docs]\n",
    "        standard_deviation = np.std(scores)\n",
    "        threshold = scores[0] - standard_deviation\n",
    "\n",
    "        filtered_docs = list(filter(lambda pair: pair[1] > threshold, docs))\n",
    "        filtered_docs = list(map(lambda pair: pair[0], filtered_docs))\n",
    "\n",
    "        if self._deduplicate:\n",
    "            existing = list()\n",
    "            for i, doc in enumerate(filtered_docs):\n",
    "                filtered_docs[i] = deduplicate(existing, doc)\n",
    "        return filtered_docs\n",
    "\n",
    "    @classmethod\n",
    "    def from_texts(cls: Type[VectorStore],\n",
    "                   texts: List[str],\n",
    "                   embedding: Embeddings,\n",
    "                   metadatas: Optional[List[dict]] = None,\n",
    "                   **kwargs: Any) -> VectorStore:\n",
    "        return StandardDevNeo4jVector(Neo4jVector.from_texts(texts, embedding, metadatas, **kwargs))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6055573d711eddad"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Vector Index Reference and QA Chain\n",
    "\n",
    "Again, here I assume you have run the code in [FHIR_GRAPHS](https://github.com/samschifman/RAG_on_FHIR/blob/main/RAG_on_FHIR_with_KG/FHIR_GRAPHS.ipynb) to create the `fhir_text` index. This code wraps that index in a Stnd Dev wrapper and passes that to the QA chain. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b29fca0b5ac828d2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "\n",
    "vector_index = StandardDevNeo4jVector(Neo4jVector.from_existing_index(\n",
    "    HuggingFaceBgeEmbeddings(model_name='BAAI/bge-small-en-v1.5'),\n",
    "    url=NEO4J_URI,\n",
    "    username=USERNAME,\n",
    "    password=PASSWORD,\n",
    "    database=DATABASE,\n",
    "    index_name='fhir_text'\n",
    "))\n",
    "\n",
    "\n",
    "vector_qa = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOllama(model=ollama_model),\n",
    "    chain_type='stuff',\n",
    "    retriever=vector_index.as_retriever(),\n",
    "    verbose=False,\n",
    "    chain_type_kwargs={'verbose': False, 'prompt': prompt}\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "844b210327f1544a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ask a Question of the RAG\n",
    "\n",
    "This cell defines a question and then asks it. It will first show what is returned from the Stnd Dev Vector Index and then ask the question of the LLM including those results in the prompt. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69017aa88c52e51b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "question = \"What are the blood pressure readings?\"\n",
    "\n",
    "show_similar(question, vector_index)\n",
    "print(' ')\n",
    "print(' ')\n",
    "\n",
    "show_answers(question, vector_qa, 1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e765215a064fc73a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Disclaimer:** Nothing provided here is guaranteed or warrantied to work. It is provided as is and has not been tested extensively. Using this notebook is at the risk of the user. \n",
    "\n",
    "Copyright &copy; 2024 Sam Schifman"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70d7285f284ef2f7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
